\section{Background \& Motivations}\label{sec:background}

\subsection{Computational Models of Notation}

Many systems implement detailed models of music explicitly or implicitly, but few of these implement detailed models of notation.\footnote{Computational models of music might entail the representation of higher-level musical entities apparent in the acts of listening and analysis but absent in the symbols of notation themselves, as determined to be creatively exigent. Programming researchers and musical artists have modeled many such extrasymbolic musical entities, such as large-scale form and transition \cite{polansky1991morphological}, \cite{uno1994temporal}, \cite{dobrian1995algorithmic}, \cite{abrams1999higher}, \cite{Yoo1983}, texture \cite{Horenstein:2004kx}, contrapuntal relationships \cite{Boenn:2009oq}, \cite{Acevedo2005}, \cite{Anders:2011kl}, \cite{Balser:1990tg}, \cite{Jones:2000hc}, \cite{uno1994temporal}, \cite{Bell:1995ij}, \cite{farbood2001analysis}, \cite{Cope:2002fv}, \cite{Laurson:2005dz}, \cite{Polansky:2011fu}, \cite{Ebcioglu:1980kl}, harmonic tension and resolution \cite{Melo2003}, \cite{Wiggins1999}, \cite{Foster:1995qa}, melody \cite{Hornel:1993mi}, \cite{Smith:1992pi}, meter \cite{Hamanaka:2005ff}, rhythm \cite{Nauert2007}, \cite{Degazio:1996lh}, \cite{Collins:2003bs}, timbre \cite{Xenakis:1991fu}, \cite{Creasey:1996ye}, \cite{Osaka2004}, temperament \cite{Seymour:2007qo}, \cite{Graf:2006il}, and ornamentation \cite{Ariza:2003zt}, \cite{Chico-Topfer:1998jl}. This work overlaps fruitfully with analysis tasks, and models of listening and cognition can enable novel methods of high-level musical structures and transformations, like dramatic direction, tension, and transition between sections \cite[108]{Collins2009}.} A system that affords a detailed model of music/composition without linking to a sufficiently detailed model of musical notation does not afford automated notation --- sufficiency, however, depends heavily on generative task. For example, if a composer requires an automated notation system to render complex rhythmic ideas that depend typographically on nested tuplets, a system that produces a notation only via a combination of MIDI and quantization must reduce rhythms to a non-hierarchical stream of event times, eliminating the temporally divisive approach of tuplet notation. For many rhythmic applications, though, MIDI suffices. 

Many automated notation systems exist to model musical notation and the act of typographical layout without explicitly affording the computational modeling of music or composition \cite{Smith:1972mw}, \cite{Nienhuys:2003ve}, \cite{Hoos:1998bd}, \cite{hamel1noteability}; many of these systems strongly imply a model of music, such as Gr\'{e}goire for Gregorian chant, Django for guitar tablature, and GOODFEEL for Braille notation \cite{2006}. In this light, feature-rich systems oriented toward classical composers, such as Finale, Sibelius, SCORE, Igor, Berlioz, and Nightingale fit into the mold of systems that model notation with genre as a primary determinant of generative task. Such a system might go so far as to enable a text-based object-oriented model of notation that automates some aspect of an otherwise point-and-click interface, as in the case of Sibelius's ManuScript scripting language \cite{Technology:qc}. 

Many models of musical notation were created for purposes of corpus-based computational musicology. Formats such as Music21, DARM, SMDL, HumDrum, and MuseData model notation with the generative task of searching through a large amount of data \cite{Selfridge-Field:1997ud}. Commercial notation software developers attempted to establish a data interchange standard for optical score recognition (NIFF) \cite{niff1995niff}; since its release in 2004, MusicXML has become a valid interchange format for over 160 applications and maintains a relatively application-agnostic status, as it was designed with the generative task of acting as an interchange format between variously tasked systems \cite{Good:2001if}. (are Igor and Berlioz commercial?)

Notation representations that underly many of these GUI-based systems often go undescribed as computer representations of notation, in favor of discussions about human-computer interaction. For example, Barker and Cantor developed an early model of music notation that underlies a four-oscilloscope GUI and describe their work entirely in terms of user interaction \cite{cantor1971computer}; likewise, discussions of modern commercial notation systems remain similarly oriented, without much awareness or criticism of the underlying computational models of notation. This results in insufficiently detailed models of notation; systems, for example, that provide models only of mensural notation and enable nonmensural notations only as modified instantiations of notations based on measures.

\subsection{The Development of Hierarchical Object Models of Notation}
Many early models of musical notation were not hierarchical, and Lejaran Hiller, in reflecting on decades of automated notation work, identifies the lack of hierarchical organization as a limitation of early work --- although Nick Collins points out that even Hiller's program PHRASE addresses the hierarchical organization of a score up to the level of a phrase, without moving beyond this mid-level musical structure to concerns of large-scale form \cite[108]{Collins2009}. 

There were several object-oriented music environments by 1990 \cite[139]{Polansky:1990fk}, most created in or inspired by the newly popular Smalltalk-80 programming language; while they facilitated the hierarchical modeling of musical abstractions, they omitted or radically simplified the hierarchical nature of common notation. For example, Glen Krasner (Xerox Systems Science Laboratory) created Machine Tongues VIII, a music system that created an object-oriented model of the score/orchestra distinction inherited from Max Mathews' Music N languages, with a simple linear model of ``partOn'' and ``partOff'' command sequences \cite{Krasner:1991uq}, omitting hierarchical organization entirely when the system produced notational output; although subsequent Machine Tongues systems introduced some hierarchical organization via ``note'' objects that inhabited ``event lists,'' systems did not attempt to model the hierarchical detail of all a traditional score's elements. Like Hiller's PHRASE program, Andreas Mahling's CompAss system organized events hierarchically up to the mid-level ``phrase'' level of musical structure \cite{Mahling:1991qf}. These systems extend Smalltalk-80 with interfaces to the MIDI communications protocol: as extensions of Smalltalk, they enabled the user to arbitrarily extend the system with new objects, creating a detailed and hierarchical model of music, usually flattened into a list of noteOn and noteOff commands to be notated or played back via MIDI interface. 

By 1989, Glendon Diener's Nutation system (written in Objective C for the NeXT computer) modeled both musical and notational structure hierarchically through the use of directed graphs \cite{Diener:1991zr}, \cite{Diener:1991ly}, \cite{Diener:1989ve}.

last section: system motivations
	

Linking our design priorities with those of previous systems by describing perceived deficiencies: evaluative priorities for previous systems: sufficiency instead of comprehensiveness, potentially evaluated by sonic result rather than notation, addressability (in HMSL and JMSL). Reintroduce generative task: late 50s through 80s were motivated locally by generative tasks of specific projects until IRCAM's Patchwork approached a generative task of broadly enabling composers; sufficiency determined by comprehensive task.